#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å°è¯´çˆ¬è™«ç¨‹åº
ä½¿ç”¨ Playwright å¤„ç†éœ€è¦ JavaScript æ¸²æŸ“çš„åŠ¨æ€ç½‘é¡µï¼Œä»ç¬”è¶£é˜ç½‘ç«™çˆ¬å–å°è¯´å†…å®¹å¹¶ä¿å­˜ä¸º TXT æ–‡ä»¶ã€‚

åŠŸèƒ½ç‰¹ç‚¹ï¼š
- è‡ªåŠ¨è·å–å°è¯´åç§°å’Œç« èŠ‚åˆ—è¡¨
- é€ç« ä¸‹è½½å°è¯´å†…å®¹
- ä¿å­˜ä¸º TXT æ–‡ä»¶ï¼Œæ–‡ä»¶åä½¿ç”¨å°è¯´åç§°
- æ”¯æŒå•é¡µåº”ç”¨ï¼ˆSPAï¼‰å’ŒåŠ¨æ€å†…å®¹åŠ è½½

ä½œè€…ï¼šAuto
åˆ›å»ºæ—¶é—´ï¼š2024
"""

from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re
import time
import os
from typing import List, Dict, Optional


class NovelSpider:
    """å°è¯´çˆ¬è™«ç±»ï¼Œä½¿ç”¨ Playwright å¤„ç†åŠ¨æ€ç½‘é¡µ"""
    
    def __init__(self, book_url: str):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        :param book_url: å°è¯´ä¸»é¡µURLï¼Œä¾‹å¦‚: https://www.57389b.sbs/#/book/1233/
        """
        self.book_url = book_url
        self.novel_name = ""
        self.chapters: List[Dict[str, str]] = []
        self.playwright = None
        self.browser = None
        self.page = None
        
    def init_browser(self) -> bool:
        """
        åˆå§‹åŒ–æµè§ˆå™¨
        
        :return: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            self.playwright = sync_playwright().start()
            self.browser = self.playwright.chromium.launch(headless=True)
            self.page = self.browser.new_page()
            return True
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–æµè§ˆå™¨å¤±è´¥: {e}")
            print("\nè¯·å…ˆå®‰è£…ä¾èµ–ï¼š")
            print("  pip install playwright")
            print("  playwright install chromium")
            return False
    
    def get_novel_info(self) -> bool:
        """
        è·å–å°è¯´ä¿¡æ¯å’Œç« èŠ‚åˆ—è¡¨
        
        :return: æ˜¯å¦æˆåŠŸè·å–
        """
        if not self.init_browser():
            return False
        
        print("ğŸ“– æ­£åœ¨è·å–å°è¯´ä¿¡æ¯...")
        try:
            # è®¿é—®å°è¯´ä¸»é¡µ
            self.page.goto(self.book_url, wait_until='networkidle', timeout=30000)
            time.sleep(3)  # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            
            # è·å–é¡µé¢æºç 
            page_content = self.page.content()
            soup = BeautifulSoup(page_content, 'html.parser')
            
            # è·å–å°è¯´åç§°
            self._extract_novel_name(soup)
            print(f"ğŸ“š å°è¯´åç§°: {self.novel_name}")
            
            # ç­‰å¾…ç« èŠ‚åˆ—è¡¨åŠ è½½
            self._wait_for_chapters()
            
            # é‡æ–°è·å–é¡µé¢å†…å®¹ï¼ˆå¯èƒ½åœ¨ç­‰å¾…åæ›´æ–°äº†ï¼‰
            page_content = self.page.content()
            soup = BeautifulSoup(page_content, 'html.parser')
            
            # æå–ç« èŠ‚åˆ—è¡¨
            if self._extract_chapters(soup):
                print(f"âœ… æ‰¾åˆ° {len(self.chapters)} ä¸ªç« èŠ‚")
                return True
            else:
                print("âŒ æœªæ‰¾åˆ°ç« èŠ‚åˆ—è¡¨")
                print(f"   é¡µé¢æ ‡é¢˜: {self.page.title()}")
                return False
                
        except Exception as e:
            print(f"âŒ è·å–å°è¯´ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _extract_novel_name(self, soup: BeautifulSoup) -> None:
        """ä»é¡µé¢ä¸­æå–å°è¯´åç§°"""
        # æ‰©å±•çš„é€‰æ‹©å™¨åˆ—è¡¨ï¼Œæ”¯æŒæ›´å¤šç½‘ç«™ç»“æ„
        title_selectors = [
            'h1',
            '.book-title',
            '#book-title',
            'title',
            '.bookname h1',
            '.bookname',
            '.book_info h1',
            '.book_info h2',
            '.book_con h1',
            '[class*="book-title"]',
            '[class*="book-name"]',
            '[class*="bookname"]',
            '[id*="bookname"]',
            '[id*="book-title"]',
        ]
        
        # æ’é™¤çš„æ–‡æœ¬ï¼ˆç½‘ç«™åç§°ç­‰ï¼‰
        exclude_texts = ['ç¬”è¶£é˜', 'å°è¯´', 'å°è¯´ç½‘', 'é¦–é¡µ', 'ç›®å½•', 'ç« èŠ‚åˆ—è¡¨']
        
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title_text = title_elem.get_text().strip()
                # è¿‡æ»¤æ‰ç½‘ç«™åç§°å’Œæ— å…³æ–‡æœ¬
                if title_text and title_text not in exclude_texts and len(title_text) > 1:
                    # å¦‚æœåŒ…å«ç½‘ç«™åç§°ï¼Œå°è¯•æå–ä¹¦åéƒ¨åˆ†
                    for exclude in exclude_texts:
                        if exclude in title_text:
                            parts = title_text.split(exclude)
                            title_text = parts[0].strip() if parts[0].strip() else (parts[1].strip() if len(parts) > 1 else title_text)
                            break
                    
                    if title_text and title_text not in exclude_texts:
                        self.novel_name = title_text
                        self.novel_name = re.sub(r'[<>:"/\\|?*]', '', self.novel_name)
                        break
        
        # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œä»titleæ ‡ç­¾æå–
        if not self.novel_name or self.novel_name in exclude_texts:
            title_tag = soup.find('title')
            if title_tag:
                title_text = title_tag.get_text().strip()
                # å°è¯•æå–ä¹¦åï¼ˆé€šå¸¸åœ¨titleçš„å‰é¢éƒ¨åˆ†ï¼‰
                # ç§»é™¤å¸¸è§çš„åˆ†éš”ç¬¦å’Œç½‘ç«™åç§°
                title_text = re.sub(r'[-_|].*$', '', title_text)  # ç§»é™¤åˆ†éš”ç¬¦åçš„å†…å®¹
                title_text = re.sub(r'^.*?[-_|]', '', title_text)  # ç§»é™¤åˆ†éš”ç¬¦å‰çš„å†…å®¹ï¼ˆå¦‚æœå‰é¢æ˜¯ç½‘ç«™åï¼‰
                
                parts = title_text.replace('_', ' ').replace('-', ' ').replace('|', ' ').split()
                for part in parts:
                    if part and part not in exclude_texts and len(part) > 1:
                        self.novel_name = part
                        break
                self.novel_name = re.sub(r'[<>:"/\\|?*]', '', str(self.novel_name))
        
        # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤åç§°ï¼ˆä»URLæå–ï¼‰
        if not self.novel_name or self.novel_name in exclude_texts:
            # å°è¯•å¤šç§URLæ ¼å¼
            book_id_match = re.search(r'/book/(\d+)/?', self.book_url) or \
                           re.search(r'/(\d+_\d+)/?', self.book_url) or \
                           re.search(r'/(\d+)/?$', self.book_url.split('/')[-2] if '/' in self.book_url else '')
            if book_id_match:
                book_id = book_id_match.group(1) if hasattr(book_id_match, 'group') else str(book_id_match)
                self.novel_name = f"å°è¯´_{book_id}"
    
    def _wait_for_chapters(self) -> None:
        """ç­‰å¾…ç« èŠ‚åˆ—è¡¨åŠ è½½"""
        # æ‰©å±•çš„é€‰æ‹©å™¨ï¼Œæ”¯æŒæ›´å¤šç½‘ç«™ç»“æ„
        selectors = [
            'a[href*="chapter"]',
            'a[href*="/chapter/"]',
            '.chapter-list a',
            '#chapter-list a',
            'dd a',
            'dt a',
            '.list-group-item a',
            'ul.list a',
            'div.list a',
            '.chapter a',
            '#list a',
            '.book_list a',
            'table a',
        ]
        
        for selector in selectors:
            try:
                self.page.wait_for_selector(selector, timeout=3000)
                break  # æ‰¾åˆ°å°±é€€å‡º
            except:
                continue  # ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ª
    
    def _extract_chapters(self, soup: BeautifulSoup) -> bool:
        """ä»é¡µé¢ä¸­æå–ç« èŠ‚åˆ—è¡¨"""
        # æ‰©å±•çš„é€‰æ‹©å™¨åˆ—è¡¨ï¼Œæ”¯æŒæ›´å¤šç½‘ç«™ç»“æ„
        chapter_selectors = [
            'a[href*="chapter"]',
            'a[href*="/chapter/"]',
            '.chapter-list a',
            '#chapter-list a',
            'dd a',                    # å¸¸è§çš„å°è¯´ç½‘ç«™ç»“æ„
            'dt a',                    # æœ‰äº›ç½‘ç«™ç”¨dtæ ‡ç­¾
            '.list-group-item a',
            'ul.list a',
            'div.list a',
            '.chapter a',
            '#list a',                 # ç« èŠ‚åˆ—è¡¨å®¹å™¨
            '.book_list a',            # ä¹¦ç±åˆ—è¡¨
            '.chapter_list a',         # ç« èŠ‚åˆ—è¡¨
            'table a',                 # è¡¨æ ¼ä¸­çš„é“¾æ¥
            'tbody a',                 # è¡¨æ ¼ä½“ä¸­çš„é“¾æ¥
            '.listmain dd a',          # å¸¸è§ç»“æ„
            '.listmain dt a',
            '#list dd a',
            '#list dt a',
        ]
        
        chapter_links = []
        for selector in chapter_selectors:
            links = soup.select(selector)
            if links and len(links) > 3:  # è‡³å°‘3ä¸ªé“¾æ¥æ‰è®¤ä¸ºæ˜¯ç« èŠ‚åˆ—è¡¨
                chapter_links = links
                print(f"   ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(links)} ä¸ªç« èŠ‚")
                break
        
        # å¦‚æœç‰¹å®šé€‰æ‹©å™¨æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨é€šç”¨æœç´¢
        if not chapter_links:
            chapter_links = self._search_chapters_generic(soup)
        
        # å»é‡å¹¶æå–ç« èŠ‚ä¿¡æ¯
        seen_urls = set()
        base_url = self.page.url.split('#')[0]
        
        for item in chapter_links:
            if isinstance(item, dict):
                url = item.get('url', '')
                title = item.get('title', '')
            else:
                url = item.get('href', '')
                title = item.get_text().strip()
                if not url.startswith('http'):
                    if url.startswith('/'):
                        url = base_url.rstrip('/') + url
                    else:
                        url = base_url.rstrip('/') + '/' + url
            
            if url and title and url not in seen_urls:
                seen_urls.add(url)
                self.chapters.append({
                    'title': title,
                    'url': url
                })
        
        return len(self.chapters) > 0
    
    def _search_chapters_generic(self, soup: BeautifulSoup) -> List:
        """é€šç”¨æ–¹æ³•æœç´¢ç« èŠ‚é“¾æ¥"""
        all_links = soup.find_all('a', href=True)
        base_url = self.page.url.split('#')[0]
        # å¤„ç†åŸºç¡€URLï¼Œç¡®ä¿æœ‰åè®®
        if not base_url.startswith('http'):
            base_url = self.book_url.split('#')[0]
        
        chapter_links = []
        exclude_texts = ['é¦–é¡µ', 'ä¸Šä¸€ç« ', 'ä¸‹ä¸€ç« ', 'ç›®å½•', 'è¿”å›', 'ä¸Šä¸€é¡µ', 'ä¸‹ä¸€é¡µ', 'åŠ å…¥ä¹¦æ¶', 'æ¨è', 'æ”¶è—']
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text().strip()
            
            # è·³è¿‡ç©ºé“¾æ¥å’Œæ’é™¤çš„æ–‡æœ¬
            if not text or text in exclude_texts:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç« èŠ‚é“¾æ¥çš„å¤šç§æ¨¡å¼
            is_chapter = False
            
            # 1. URLæ¨¡å¼æ£€æŸ¥
            if (
                'chapter' in href.lower() or
                '/book/' in href or
                re.search(r'/\d+\.html', href) or
                re.search(r'/\d+_\d+\.html', href) or  # å¦‚: 8_8426/12345.html
                re.search(r'/\d+/\d+\.html', href) or  # å¦‚: 8/8426/12345.html
                re.search(r'/\d+/\d+\.htm', href) or
                re.search(r'chapter/\d+', href, re.I) or
                re.search(r'/\d+\.htm', href)
            ):
                is_chapter = True
            
            # 2. æ–‡æœ¬æ¨¡å¼æ£€æŸ¥
            if (
                re.search(r'ç¬¬.*ç« ', text) or
                re.search(r'ç¬¬.*èŠ‚', text) or
                re.search(r'^\d+[ã€.]', text) or
                re.search(r'^\d+\s+', text) or
                re.search(r'ç¬¬\d+ç« ', text) or
                re.search(r'ç¬¬\d+èŠ‚', text)
            ):
                is_chapter = True
            
            # 3. é•¿åº¦å’Œæ ¼å¼æ£€æŸ¥ï¼ˆæ’é™¤å¯¼èˆªé“¾æ¥ï¼‰
            if not is_chapter and len(text) < 50 and len(text) > 2:
                # å¦‚æœé“¾æ¥åœ¨ç‰¹å®šçš„å®¹å™¨ä¸­ï¼ˆå¦‚ç« èŠ‚åˆ—è¡¨åŒºåŸŸï¼‰
                parent = link.parent
                if parent:
                    parent_class = parent.get('class', [])
                    parent_id = parent.get('id', '')
                    if any(keyword in str(parent_class).lower() or keyword in parent_id.lower() 
                           for keyword in ['list', 'chapter', 'book', 'content']):
                        is_chapter = True
            
            if is_chapter:
                # æ„å»ºå®Œæ•´URL
                if not href.startswith('http'):
                    if href.startswith('/'):
                        href = base_url.rstrip('/') + href
                    elif href.startswith('./') or href.startswith('../'):
                        # å¤„ç†ç›¸å¯¹è·¯å¾„
                        href = urljoin(base_url, href)
                    else:
                        # ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦æ‹¼æ¥
                        if base_url.endswith('/'):
                            href = base_url + href
                        else:
                            href = base_url + '/' + href
                
                chapter_links.append({
                    'title': text,
                    'url': href,
                    'element': link
                })
        
        if chapter_links:
            print(f"   é€šè¿‡é€šç”¨æœç´¢æ‰¾åˆ° {len(chapter_links)} ä¸ªå¯èƒ½çš„ç« èŠ‚é“¾æ¥")
        
        return chapter_links
    
    def get_chapter_content(self, chapter_url: str) -> Optional[str]:
        """
        è·å–ç« èŠ‚å†…å®¹
        
        :param chapter_url: ç« èŠ‚URL
        :return: ç« èŠ‚å†…å®¹æ–‡æœ¬ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            self.page.goto(chapter_url, wait_until='networkidle', timeout=20000)
            time.sleep(1)  # ç­‰å¾…å†…å®¹åŠ è½½
            
            page_content = self.page.content()
            soup = BeautifulSoup(page_content, 'html.parser')
            
            # æ‰©å±•çš„å†…å®¹é€‰æ‹©å™¨ï¼Œæ”¯æŒæ›´å¤šç½‘ç«™ç»“æ„
            content_selectors = [
                '#content',
                '.content',
                '#chaptercontent',
                '.chapter-content',
                '#novelcontent',
                '.text-content',
                '#text',
                '#chaptercontent',
                '.chaptercontent',
                '.bookcontent',
                '#bookcontent',
                '.novelcontent',
                '#novelcontent',
                '[id*="content"]',
                '[class*="content"]',
                '[id*="text"]',
                '[class*="text"]',
                '[id*="chapter"]',
                '[class*="chapter"]',
                '.readcontent',
                '#readcontent',
            ]
            
            content = None
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    text = content_elem.get_text().strip()
                    if len(text) > 200:  # å†…å®¹åº”è¯¥è¶³å¤Ÿé•¿
                        content = text
                        break
            
            # å¦‚æœç‰¹å®šé€‰æ‹©å™¨æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨é€šç”¨æœç´¢
            if not content:
                # å°è¯•æŸ¥æ‰¾åŒ…å«å¤§é‡æ–‡æœ¬çš„å…ƒç´ 
                # ä¼˜å…ˆæŸ¥æ‰¾divï¼Œç„¶åæ˜¯å…¶ä»–å—çº§å…ƒç´ 
                elements = soup.find_all(['div', 'article', 'section'], 
                                        class_=re.compile(r'content|text|chapter|novel|read|book', re.I))
                
                # å¦‚æœæ²¡æ‰¾åˆ°ï¼ŒæŸ¥æ‰¾æ‰€æœ‰div
                if not elements:
                    elements = soup.find_all('div')
                
                for elem in elements:
                    text = elem.get_text().strip()
                    # å†…å®¹åº”è¯¥è¶³å¤Ÿé•¿ï¼Œä¸”ä¸åŒ…å«å¤ªå¤šé“¾æ¥ï¼ˆæ’é™¤å¯¼èˆªåŒºåŸŸï¼‰
                    links_count = len(elem.find_all('a'))
                    if len(text) > 500 and links_count < 10:  # å†…å®¹é•¿ä¸”é“¾æ¥å°‘
                        content = text
                        break
            
            if content:
                # æ¸…ç†å†…å®¹
                content = re.sub(r'\s+', '\n', content)
                content = re.sub(r'\n{3,}', '\n\n', content)  # å¤šä¸ªæ¢è¡Œæ›¿æ¢ä¸ºä¸¤ä¸ª
                content = content.strip()
            
            return content
        except Exception as e:
            print(f"   è·å–ç« èŠ‚å†…å®¹å¤±è´¥: {e}")
            return None
    
    def save_novel(self, output_dir: str = 'novels') -> bool:
        """
        ä¿å­˜å°è¯´åˆ°txtæ–‡ä»¶
        
        :param output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º 'novels'
        :return: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        if not self.novel_name or not self.chapters:
            print("âŒ å°è¯´ä¿¡æ¯ä¸å®Œæ•´ï¼Œæ— æ³•ä¿å­˜")
            return False
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        filename = os.path.join(output_dir, f"{self.novel_name}.txt")
        
        print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½ç« èŠ‚å†…å®¹...")
        print(f"   ä¿å­˜è·¯å¾„: {filename}")
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                # å†™å…¥å°è¯´æ ‡é¢˜
                f.write(f"{self.novel_name}\n\n")
                f.write("=" * 50 + "\n\n")
                
                # éå†æ‰€æœ‰ç« èŠ‚
                total = len(self.chapters)
                for idx, chapter in enumerate(self.chapters, 1):
                    print(f"   [{idx}/{total}] {chapter['title']}")
                    
                    # å†™å…¥ç« èŠ‚æ ‡é¢˜
                    f.write(f"\n\n{chapter['title']}\n\n")
                    
                    # è·å–ç« èŠ‚å†…å®¹
                    content = self.get_chapter_content(chapter['url'])
                    if content:
                        f.write(content)
                        f.write("\n")
                    else:
                        f.write("[å†…å®¹è·å–å¤±è´¥]\n")
                    
                    # å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(0.5)
            
            print(f"\nâœ… ä¸‹è½½å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜åˆ°: {filename}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return False
        finally:
            # æ¸…ç†èµ„æº
            self._cleanup()
    
    def _cleanup(self) -> None:
        """æ¸…ç†æµè§ˆå™¨èµ„æº"""
        try:
            if self.browser:
                self.browser.close()
            if self.playwright:
                self.playwright.stop()
        except:
            pass


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å°è¯´URLï¼ˆå¯ä»¥ä¿®æ”¹ä¸ºå…¶ä»–å°è¯´ï¼‰
    # æ”¯æŒå¤šç§ç½‘ç«™æ ¼å¼ï¼š
    # - https://www.57389b.sbs/#/book/1233/
    # - http://www.xbiqushu.com/8_8426/
    book_url = "http://www.xbiqushu.com/8_8426/"  # å¯ä»¥ä¿®æ”¹ä¸ºå…¶ä»–å°è¯´URL
    
    print("=" * 60)
    print("ğŸ“š å°è¯´çˆ¬è™«ç¨‹åº")
    print("=" * 60)
    print(f"ç›®æ ‡URL: {book_url}")
    print()
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    spider = NovelSpider(book_url)
    
    # è·å–å°è¯´ä¿¡æ¯å¹¶ä¿å­˜
    if spider.get_novel_info():
        spider.save_novel()
    else:
        print("\nâŒ è·å–å°è¯´ä¿¡æ¯å¤±è´¥")
        print("\nå¯èƒ½çš„åŸå› ï¼š")
        print("  1. ç½‘ç»œè¿æ¥é—®é¢˜")
        print("  2. URLä¸æ­£ç¡®")
        print("  3. ç½‘ç«™ç»“æ„å‘ç”Ÿå˜åŒ–")
        print("  4. éœ€è¦ç™»å½•æˆ–éªŒè¯")
        print("\næç¤ºï¼š")
        print("  - ç¡®ä¿URLæ˜¯å°è¯´ç›®å½•é¡µï¼ˆåŒ…å«ç« èŠ‚åˆ—è¡¨çš„é¡µé¢ï¼‰")
        print("  - å¯ä»¥å°è¯•åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€URLï¼Œç¡®è®¤é¡µé¢æ­£å¸¸æ˜¾ç¤º")


if __name__ == '__main__':
    main()
