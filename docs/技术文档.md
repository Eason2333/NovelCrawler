# 技术文档

## 目录

1. [项目概述](#项目概述)
2. [技术架构](#技术架构)
3. [核心实现](#核心实现)
4. [关键技术点](#关键技术点)
5. [扩展开发](#扩展开发)

## 项目概述

### 项目目标

开发一个能够从动态网站（单页应用）爬取小说内容的爬虫程序，支持：
- 自动识别小说名称和章节列表
- 批量下载所有章节内容
- 保存为格式化的 TXT 文件

### 技术选型

| 技术 | 选择 | 原因 |
|------|------|------|
| 浏览器自动化 | Playwright | 自动下载浏览器，配置简单，功能强大 |
| HTML解析 | BeautifulSoup4 | 简单易用，解析效率高 |
| 编程语言 | Python 3.7+ | 语法简洁，生态丰富 |

## 技术架构

### 系统架构图

```
┌─────────────────┐
│   用户输入URL    │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  NovelSpider     │
│  初始化浏览器    │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  访问小说主页   │
│  等待JS渲染     │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  提取小说信息   │
│  - 小说名称      │
│  - 章节列表      │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  逐章下载内容   │
│  保存到TXT      │
└─────────────────┘
```

### 类结构设计

```python
NovelSpider
├── __init__()              # 初始化
├── init_browser()          # 初始化浏览器
├── get_novel_info()        # 获取小说信息（主入口）
│   ├── _extract_novel_name()    # 提取小说名称
│   ├── _wait_for_chapters()    # 等待章节加载
│   └── _extract_chapters()     # 提取章节列表
│       └── _search_chapters_generic()  # 通用搜索
├── get_chapter_content()   # 获取章节内容
└── save_novel()           # 保存小说
    └── _cleanup()         # 清理资源
```

## 核心实现

### 1. 浏览器初始化

```python
def init_browser(self) -> bool:
    """使用 Playwright 启动无头浏览器"""
    self.playwright = sync_playwright().start()
    self.browser = self.playwright.chromium.launch(headless=True)
    self.page = self.browser.new_page()
```

**关键点：**
- `headless=True`: 无头模式，不显示浏览器窗口
- 使用 Chromium 浏览器（轻量级）

### 2. 页面加载和等待

```python
self.page.goto(self.book_url, wait_until='networkidle', timeout=30000)
time.sleep(3)  # 额外等待确保内容加载
```

**关键点：**
- `wait_until='networkidle'`: 等待网络请求完成
- `timeout=30000`: 30秒超时
- 额外等待3秒确保 JavaScript 完全执行

### 3. 小说名称提取

**策略：**
1. 优先使用特定选择器（h1, .book-title 等）
2. 从 title 标签提取
3. 如果都失败，使用默认名称（从URL提取ID）

**代码逻辑：**
```python
def _extract_novel_name(self, soup: BeautifulSoup) -> None:
    # 1. 尝试特定选择器
    for selector in title_selectors:
        title_elem = soup.select_one(selector)
        if title_elem and title_elem.get_text() != '笔趣阁':
            self.novel_name = title_elem.get_text()
            break
    
    # 2. 从title标签提取
    if not self.novel_name:
        # 解析title标签...
    
    # 3. 使用默认名称
    if not self.novel_name:
        self.novel_name = f"小说_{book_id}"
```

### 4. 章节列表提取

**策略：**
1. 优先使用特定选择器（.chapter-list a 等）
2. 通用搜索所有链接
3. 过滤和验证章节链接

**过滤条件：**
- URL 包含 'chapter' 或 '/book/'
- 文本包含 '第X章'
- 文本以数字开头
- 排除导航链接（首页、上一章等）

### 5. 章节内容提取

**策略：**
1. 尝试常见的内容选择器（#content, .content 等）
2. 搜索包含大量文本的 div
3. 内容清理和格式化

**内容清理：**
```python
content = re.sub(r'\s+', '\n', content)      # 多个空白字符替换为换行
content = re.sub(r'\n{3,}', '\n\n', content)  # 多个换行替换为两个
```

## 关键技术点

### 1. 单页应用（SPA）处理

**问题：**
- 网站使用 `#/` 路由
- 内容通过 JavaScript 动态加载
- 初始 HTML 可能是空容器

**解决方案：**
- 使用 Playwright 等待 JavaScript 执行
- `wait_until='networkidle'` 等待网络请求完成
- 额外等待时间确保内容渲染

### 2. 选择器策略

**多级回退机制：**
1. 特定选择器（针对常见网站结构）
2. 通用搜索（适用于各种结构）
3. 智能过滤（排除非章节链接）

### 3. 错误处理

**分层错误处理：**
- 浏览器初始化失败 → 提示安装步骤
- 页面加载失败 → 打印错误信息
- 章节获取失败 → 标记为失败，继续下载其他章节

### 4. 资源管理

**自动清理：**
```python
finally:
    if self.browser:
        self.browser.close()
    if self.playwright:
        self.playwright.stop()
```

确保即使出错也能正确释放资源。

## 扩展开发

### 1. 支持更多网站

**修改点：**
1. 调整章节选择器（`_extract_chapters`）
2. 调整内容选择器（`get_chapter_content`）
3. 可能需要调整等待时间

**示例：**
```python
# 针对特定网站添加选择器
chapter_selectors = [
    'a[href*="chapter"]',  # 通用
    '.chapter-list a',     # 网站A
    '.book-chapters a',     # 网站B（新增）
]
```

### 2. 添加进度保存

**功能：**
- 支持断点续传
- 保存下载进度
- 跳过已下载章节

**实现思路：**
```python
# 检查已下载的章节
downloaded = set()
if os.path.exists(filename):
    # 读取已下载章节...
    
# 跳过已下载
for chapter in self.chapters:
    if chapter['url'] not in downloaded:
        # 下载...
```

### 3. 添加多线程下载

**注意：**
- 需要控制并发数
- 避免对服务器造成压力
- 注意线程安全

### 4. 添加配置文件

**配置项：**
- 输出目录
- 请求延迟
- 选择器配置
- 超时时间

**实现：**
```python
# config.json
{
    "output_dir": "novels",
    "delay": 0.5,
    "timeout": 30000,
    "selectors": {
        "chapter": [".chapter-list a"],
        "content": ["#content"]
    }
}
```

### 5. 添加日志系统

**功能：**
- 记录下载进度
- 记录错误信息
- 支持日志级别

**实现：**
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('spider.log'),
        logging.StreamHandler()
    ]
)
```

## 性能优化

### 1. 减少等待时间

- 根据实际情况调整 `time.sleep()`
- 使用更精确的等待条件

### 2. 批量处理

- 可以批量获取多个章节的URL
- 减少页面跳转次数

### 3. 缓存机制

- 缓存已解析的页面结构
- 避免重复解析

## 调试技巧

### 1. 查看页面内容

```python
# 保存页面HTML用于调试
with open('debug.html', 'w', encoding='utf-8') as f:
    f.write(self.page.content())
```

### 2. 打印调试信息

```python
print(f"找到 {len(self.chapters)} 个章节")
print(f"页面标题: {self.page.title()}")
```

### 3. 使用非无头模式

```python
# 临时改为有头模式，查看浏览器行为
self.browser = self.playwright.chromium.launch(headless=False)
```

## 常见问题解决

### 1. 章节列表为空

**可能原因：**
- 选择器不正确
- 等待时间不足
- 网站结构变化

**解决方法：**
1. 增加等待时间
2. 调整选择器
3. 使用通用搜索

### 2. 内容获取失败

**可能原因：**
- 内容选择器不正确
- 内容被JavaScript动态加载
- 需要登录

**解决方法：**
1. 检查页面HTML结构
2. 调整内容选择器
3. 增加等待时间

### 3. 下载速度慢

**优化方法：**
1. 减少等待时间（但要保证稳定性）
2. 使用并发下载（需谨慎）
3. 优化选择器，减少DOM查询

---

**最后更新：** 2024年

